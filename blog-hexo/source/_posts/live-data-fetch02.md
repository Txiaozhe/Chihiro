---
title: 复盘1：直播大数据采集（二期）
date: '2019-12-26 21:35:14'
# photos: ./1.jpeg
abstract: 复盘一下以前做过的事情
categories:
- Tech
tags:
- Job
- Live
- BigData
# top: 1
---

上一篇文章《直播大数据采集（一期）》（以下简称《一期》）复盘了之前做过的一个采集直播大数据项目，其中介绍了项目相关的细节和技术实现始末，也描述了当时碰到的问题以及采用的解决方案，但由于涉及到太多导致篇幅过长，且当时项目迭代也是分为两期来做的，所以在这里将整个项目分为两篇文章来描述。本文即承接上文介绍当时由于业务需求和技术迭代所做的二期改造过程。

一期项目中已对初始的业务需求进行了实现，如直播大数据采集、数据分析、平台展示等。当时整个采集系统采集的直播间数量峰值为 10 万个，集群规模为 40 节点，也就是高峰时平均每节点要承担 2500 个直播间的采集和数据清洗任务，而每天流过整个系统的总流量为 400 ~ 600 G，集群采用 ansible 进行统一部署，使用 alinode 和 Grafana 进行监控和管理。且后期使用爬虫代理对风控较严格的平台进行采集，使得数据流趋于稳定，基本能满足业务需求。于是开始着手准备二期升级和迭代。

工程上的任何一次迭代都应该基于现实业务或解决某个问题，此次迭代也正由于需求方提出了新的需求以及我们也希望通过这次迭代能用更好的方式解决一期项目中遇到的问题。整理当时接到的新需求以及待解决的问题如下：

* **”原始数据“ 需求**：当时部门内一个客户端团队（我当时处于数据团队）在设计客户端时，涉及到在客户端中展示主播在播时的弹幕和礼物，考虑到数据团队已经能采集到完整的直播流数据，因此他们希望能对接采集到的数据，而且最好是原始数据（在当时的架构中最终输出的数据已经经过去平台化的清洗）。
* **优化本地数据清洗**：一期项目中数据的去平台化清洗任务是在集群节点上进行的，其中涉及到大量的数据转换和计算，这加大了集群节点的负担。
* **关播后延迟关闭采集**：在《一期》中已描述过，当一个直播间关播后，集群会自动关闭对该直播间的采集。延迟关闭也就是说当主播关播时，集群会延迟关闭对该直播间的采集。
* **对部分热门主播进行长期采集**：对部分热门的网红直播间进行长期不间断的采集，但直播间开关播信息依然需要正常发出。
* **调度服务的单点问题**：集群服务都会关注的单点问题！

## 架构重构

### 设计

基于以上提出的新需求以及问题，其中涉及到大量与数据相关的内容，这势必要对一期项目的架构进行重新设计。考虑到未来可能会有需求的扩展，且需要对整体系统进行解耦，决定采用普遍且适应性强的分层式设计的新架构方案。

如下图，整个系统维持原有采集架构不变，在这里分为数据层和应用层两大块。

![](/images/live-data-fetch2-fenceng.png)

* **数据层**：即用于接收和处理所有数据的模块。将原先分布于采集结点中的数据清洗过程独立出来进行重新设计，又将该过程分为三个阶段：原始数据、ETL（数据清洗）、大数据计算。

* * **原始数据**：从采集集群采集到的数据经过序列化后得到的初始数据，这些数据具有强烈的平台特色，因此这个阶段得到的数据在字段、类型、名称等方面存在很大差异。但某些特殊业务场景下也会需要该数据。
  * **ETL**：上述原始数据由于存在平台差异，因此无法进行统一计算和分析，因此在这一阶段需要对数据进行差异化抹平，即去平台化，以便后续进行统一计算和分析。
  * **大数据计算**：在该阶段根据特定业务需求对大数据进行分析计算得出结果，并提供给业务使用。
* **MQ（消息队列）**：作为沟通原始数据、ETL 和大数据计算三个阶段沟通的桥梁，原始数据获得后将数据直接发送至 MQ 中，而 ETL 和大数据计算阶段则可以根据需要对消息队列进行消费，且 ETL 阶段清洗后的数据也会被传至 MQ 中。
  

数据层负责对大数据的各种处理，一方面每一阶段都可以独立提供对外的响应接口，因此每一阶段得到的数据都能被单独应用，这使得未来业务具有更加强大的可扩展性，另一方面数据分为多个阶段来处理的方式与一期相比，系统达到了解耦的目的，且降低了集群节点的负担，各个阶段之间通过 MQ 交互，将整个过程异步化，本身相互依赖度也更加低，提高了数据处理效率的同时也降低了开发维护成本。

* **应用层**：这一层主要是应用数据层提供的各种数据能力，包括但不限于各种业务服务、日志、存储相关部分，各个应用之间相互独立，也可单独部署。单独把数据应用独立出来可以增强整体系统的可扩展性，也极大地降低了基础设施与上层业务之间的耦合度。

### 技术实现

根据上述架构设计方案，可以整理出各个部分需要做的开发工作。原始数据部分可以继承一期中的采集阶段，将节点采集到数据后所做的数据清洗操作移除，并直接将数据推送至消息队列。消息队列仍然采用阿里云 Loghub（虽然看上去不合理但是在当时属于白菜价而且后续大数据计算操作对接非常顺畅）。

大数据处理部分由于直接使用了阿里云的流计算和实时计算服务并由另一位同学单独负责，且该服务支持直接从 Loghub 中消费数据，处理起来除业务部分并不复杂，当时阿里云的技术支持也比较到位，所以不在此展开详述。

ETL 部分会比较复杂一些，且需要重新开发。梳理一下其中可能会碰到的问题：

* **数据量大**：每天 400 ~ 600G。
* **业务复杂**：涉及到多个平台的不同种类数据，其中数据格式、字段名称含义、类型等都不一样，为最终抹平各平台间的差异需要做去平台化操作，并且市面上也找不到现成的清洗方案。
* **计算量大**：一方面数据量本身较大，另一方面业务中会涉及到大量取值、转换、累计、求平均等数值计算操作，原有的 Node.js 方案（原本整体系统都是基于 Node.js 来实现）在这里可能并不适用。

基于上述问题，并在团队讨论后考虑在 ETL 的实现中放弃现有的 Node.js 体系，使用 Go 语言针对该特定场景开发一套数据清洗系统，使用 Go 语言主要考虑因素如下：

* **性能**：Go 语言作为一门编译型语言，虽然不如 C/C++ 那样效率超群，但对比 JavaScript 这类脚本语言还是绰绰有余的。且放弃现有 Node.js 体系的原因也主要是 Node.js 的本身设计因素决定了其并不适合用于大量 CPU 计算，与现有场景相悖，采用 Go 语言实现也能提高数值计算的效率。
* **学习成本低**：作为业务工程化采用的技术方案，必然考虑的一个因素就是可持续维护，Go 语言的学习成本与 C/C++ 相比低太多，当项目人员流失时也能快速找到替补。
* **生态良好**：Go 的语言生态持续完善，且市场上已有大量使用 Go 语言实现的优秀开源工具，如 Kubernetes、Docker。

采用新的架构方案并进行了相关优化的实现后，整体业务趋于平稳，架构中各数据层也能独立稳定地对外提供数据服务。将数据清洗操作从原有集群节点中拆分出来后也极大降低了节点负担，优化后的系统整体达到了架构设计预期的效果。

## 实现新需求











为什么用loghub作为消息队列